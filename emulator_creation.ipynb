{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from tpot.export_utils import set_param_recursive\n",
    "import xarray as xr\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import dask\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SGECluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SGECluster(\n",
    "    walltime='02:00:00', \n",
    "    memory='1 G',\n",
    "    resource_spec='h_vmem=2G',\n",
    "    scheduler_options={\n",
    "        'dashboard_address': ':5757',\n",
    "    },\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup and run emulator and sensitivity analysis - pangeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emulator setup\n",
    "path = '/nobackup/earlacoa/machinelearning/data/'\n",
    "\n",
    "with open(path + 'dict_train.pickle', 'rb') as ds:\n",
    "    dict_train = pickle.load(ds)\n",
    "    \n",
    "with open(path + 'dict_test.pickle', 'rb') as ds:\n",
    "    dict_test = pickle.load(ds)\n",
    "    \n",
    "df_train = pd.concat(dict_train, ignore_index=True)\n",
    "df_test = pd.concat(dict_test, ignore_index=True)\n",
    "\n",
    "inputs_train = pd.read_csv(path + 'latin_hypercube_inputs_train.csv')\n",
    "inputs_test = pd.read_csv(path + 'latin_hypercube_inputs_test.csv')\n",
    "X_train, X_test = inputs_train.values, inputs_test.values\n",
    "\n",
    "lats = df_train[['lat', 'lon']].drop_duplicates()['lat'].values\n",
    "lons = df_train[['lat', 'lon']].drop_duplicates()['lon'].values\n",
    "\n",
    "df_eval_summary = pd.DataFrame(columns=['output', 'rmse_cv', 'r2_cv', 'rmse_test', 'r2_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity analysis setup\n",
    "sims = ['RES', 'IND', 'TRA', 'AGR', 'ENE']\n",
    "sens_inds_S1_ST = ['S1', 'S1_conf', 'ST', 'ST_conf']\n",
    "ds_sens_ind = xr.Dataset({})\n",
    "\n",
    "empty_values = np.empty((580, 1440))\n",
    "empty_values[:] = np.nan\n",
    "empty_da = xr.DataArray(empty_values, dims=('lat', 'lon'), coords={'lat': np.arange(-60, 85, 0.25), 'lon': np.arange(-180, 180, 0.25)})\n",
    "\n",
    "for sim in sims:\n",
    "    for sens_ind in sens_inds_S1_ST:\n",
    "        ds_sens_ind.update({sens_ind + '_' + sim: empty_da})\n",
    "        \n",
    "sims_S2 = ['RES_IND', 'RES_TRA', 'RES_AGR', 'RES_ENE', 'IND_TRA', 'IND_AGR', 'IND_ENE', 'TRA_AGR', 'TRA_ENE', 'AGR_ENE']\n",
    "sims_S2_indexes = [(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
    "sens_inds_S2 = ['S2', 'S2_conf']\n",
    "\n",
    "for sim in sims_S2:\n",
    "    for sens_ind in sens_inds_S2:\n",
    "        ds_sens_ind.update({sens_ind + '_' + sim: empty_da})\n",
    "\n",
    "sens_inputs = {\n",
    "    'num_vars': 5,\n",
    "    'names': sims,\n",
    "    'bounds': [[0.0, 1.5],\n",
    "               [0.0, 1.5],\n",
    "               [0.0, 1.5],\n",
    "               [0.0, 1.5],\n",
    "               [0.0, 1.5]]\n",
    "}\n",
    "\n",
    "sens_param_values = saltelli.sample(sens_inputs, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_values(gridcell, df_train, df_test, output):\n",
    "    \"\"\"for a given gridcell, return the training and test data\"\"\"\n",
    "    lat, lon = gridcell\n",
    "    \n",
    "    df_train_gridcell = df_train.loc[df_train.lat == lat].loc[df_train.lon == lon]\n",
    "    df_test_gridcell = df_test.loc[df_test.lat == lat].loc[df_test.lon == lon]\n",
    "    \n",
    "    y_train, y_test = df_train_gridcell[output].values, df_test_gridcell[output].values\n",
    "    \n",
    "    return lat, lon, y_train, y_test\n",
    "\n",
    "def create_emulator():\n",
    "    \"\"\"create a new gaussian process emulator\"\"\"\n",
    "    emulator = make_pipeline(\n",
    "        PowerTransformer(),\n",
    "        GaussianProcessRegressor(\n",
    "            kernel=Matern(length_scale=3.4000000000000004, nu=2.5), \n",
    "            n_restarts_optimizer=240, \n",
    "            normalize_y=False)\n",
    "    )\n",
    "    set_param_recursive(emulator.steps, 'random_state', 123)\n",
    "    \n",
    "    return emulator\n",
    "\n",
    "\n",
    "def emulator_cv(emulator, X_train, y_train, y_test):\n",
    "    \"\"\"10-fold cross-validation on the emulator using the training data\"\"\"\n",
    "    cv = cross_validate(emulator, X_train, y_train, cv=10, scoring={'r2': 'r2', 'rmse': 'neg_mean_squared_error'})\n",
    "    \n",
    "    return cv\n",
    "\n",
    "\n",
    "def emulator_fit_save(emulator, X_train, y_train, path, output, lat, lon):\n",
    "    \"\"\"fit the emulator to the training data and save\"\"\"\n",
    "    emulator.fit(X_train, y_train)\n",
    "        \n",
    "    joblib.dump(emulator, path + output + '/emulator_' + output + '_' + str(lat) + '_' + str(lon) + '.joblib')\n",
    "    \n",
    "    return emulator\n",
    "    \n",
    "    \n",
    "def sensitivity_analysis(sens_inputs, sens_predictions):\n",
    "    \"\"\"determine the sensitivity indices of the emulator\"\"\"\n",
    "    sens_ind_dict = sobol.analyze(sens_inputs, sens_predictions)\n",
    "    \n",
    "    return sens_ind_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emulator_with_sensitivity(gridcell):\n",
    "    \"\"\"run all the functions to create the emulator and run the sensitivity analysis\"\"\"\n",
    "    lat, lon, y_train, y_test = output_values(gridcell, df_train, df_test, output)\n",
    "    \n",
    "    emulator = create_emulator()\n",
    "    \n",
    "    cv = emulator_cv(emulator, X_train, y_train, y_test)\n",
    "    r2_cv = cv['test_r2']\n",
    "    rmse_cv = np.sqrt(np.abs(cv['test_rmse']))\n",
    "    \n",
    "    emulator = emulator_fit_save(emulator, X_train, y_train, path, output, lat, lon)\n",
    "    \n",
    "    y_pred = emulator.predict(X_test)\n",
    "    \n",
    "    sens_predictions = emulator.predict(sens_param_values)\n",
    "    sens_ind_dict = sensitivity_analysis(sens_inputs, sens_predictions)\n",
    "\n",
    "    return lat, lon, output, y_test, y_pred, rmse_cv, r2_cv, sens_ind_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'PM2_5_DRY'\n",
    "# 2 hours\n",
    "# 'PM2_5_DRY', 'o3', 'bc_2p5', 'oc_2p5', 'no3_2p5', 'oin_2p5'\n",
    "# 3 hours\n",
    "# 'AOD550_sfc', 'bsoaX_2p5', 'nh4_2p5', 'no3_2p5'\n",
    "# 5 hours\n",
    "# 'asoaX_2p5'\n",
    "\n",
    "gridcells = df_train[['lat', 'lon']].drop_duplicates().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_gridcells = db.from_sequence(gridcells)\n",
    "bag_gridcells = bag_gridcells.map(emulator_with_sensitivity)\n",
    "bag_gridcells.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = bag_gridcells.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([result[3] for result in results]).ravel()\n",
    "y_pred = np.array([result[4] for result in results]).ravel()\n",
    "np.savez_compressed(path + output + '/y_test_pred_' + output + '.npz', y_test=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv = np.mean(np.array([result[5] for result in results]))\n",
    "r2_cv = np.mean(np.array([result[6] for result in results]))\n",
    "rmse_test = np.round(np.sqrt(np.abs(mean_squared_error(y_test, y_pred))), decimals=4)\n",
    "r2_test = np.round(r2_score(y_test, y_pred), decimals=4)\n",
    "\n",
    "df_eval_summary = df_eval_summary.append([{\n",
    "        'output': output,\n",
    "        'rmse_cv': rmse_cv, \n",
    "        'r2_cv': r2_cv,                                             \n",
    "        'rmse_test': rmse_test, \n",
    "        'r2_test': r2_test}],              \n",
    "        ignore_index=True, \n",
    "        sort=False)\n",
    "df_eval_summary.to_csv(path + output + '/df_eval_summary_' + output + '.csv')\n",
    "df_eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array([result[0] for result in results]).ravel()\n",
    "lons = np.array([result[1] for result in results]).ravel()\n",
    "sensitivities = [result[7] for result in results]\n",
    "\n",
    "for index, sens in enumerate(sensitivities):\n",
    "    lat = lats[index]\n",
    "    lon = lons[index]\n",
    "\n",
    "    for sim_index, sim in enumerate(sims):\n",
    "        for sens_ind_index, sens_ind in enumerate(sens_inds_S1_ST):\n",
    "            ds_sens_ind[sens_ind + '_' + sim] = xr.where(\n",
    "                (ds_sens_ind.coords['lat'] == lat) & (ds_sens_ind.coords['lon'] == lon),\n",
    "                sens[sens_ind][sim_index],\n",
    "                ds_sens_ind[sens_ind + '_' + sim]\n",
    "            )\n",
    "\n",
    "    for sim_index, sim in enumerate(sims_S2):\n",
    "        for sens_ind_index, sens_ind in enumerate(sens_inds_S2):\n",
    "            ds_sens_ind[sens_ind + '_' + sim] = xr.where(\n",
    "                (ds_sens_ind.coords['lat'] == lat) & (ds_sens_ind.coords['lon'] == lon),\n",
    "                sens[sens_ind][sims_S2_indexes[sim_index][0], sims_S2_indexes[sim_index][1]],\n",
    "                ds_sens_ind[sens_ind + '_' + sim]\n",
    "            )\n",
    "\n",
    "ds_sens_ind.to_netcdf(path + output + '/ds_sens_ind_' + output + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
