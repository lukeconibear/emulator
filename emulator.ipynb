{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from tpot.export_utils import set_param_recursive\n",
    "import xarray as xr\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import dask\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from cartopy.io.shapereader import Reader\n",
    "params = {\n",
    "    'text.latex.preamble': ['\\\\usepackage{gensymb}'],\n",
    "    'axes.grid': False,\n",
    "    'savefig.dpi': 700,\n",
    "    'font.size': 12,\n",
    "    'text.usetex': False,\n",
    "    'figure.figsize': [5, 5],\n",
    "    'font.family': 'serif',\n",
    "}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SGECluster\n",
    "from dask.distributed import Client\n",
    "cluster = SGECluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=150)\n",
    "#cluster.adapt(minimum_jobs=0, maximum_jobs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emulator setup\n",
    "path = '/nobackup/earlacoa/machinelearning/data/'\n",
    "\n",
    "with open(path + 'dict_train.pickle', 'rb') as ds:\n",
    "    dict_train = pickle.load(ds)\n",
    "    \n",
    "with open(path + 'dict_test.pickle', 'rb') as ds:\n",
    "    dict_test = pickle.load(ds)\n",
    "    \n",
    "df_train = pd.concat(dict_train, ignore_index=True)\n",
    "df_test = pd.concat(dict_test, ignore_index=True)\n",
    "\n",
    "inputs_train = pd.read_csv(path + 'latin_hypercube_inputs_train.csv')\n",
    "inputs_test = pd.read_csv(path + 'latin_hypercube_inputs_test.csv')\n",
    "X_train, X_test = inputs_train.values, inputs_test.values\n",
    "\n",
    "lats = df_train[['lat', 'lon']].drop_duplicates()['lat'].values\n",
    "lons = df_train[['lat', 'lon']].drop_duplicates()['lon'].values\n",
    "\n",
    "df_eval_summary = pd.DataFrame(columns=['output', 'rmse_cv', 'r2_cv', 'rmse_test', 'r2_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity analysis setup\n",
    "sims = ['RES', 'IND', 'TRA', 'AGR', 'ENE']\n",
    "sens_inds_S1_ST = ['S1', 'S1_conf', 'ST', 'ST_conf']\n",
    "ds_sens_ind = xr.Dataset({})\n",
    "\n",
    "empty_values = np.empty((580, 1440))\n",
    "empty_values[:] = np.nan\n",
    "empty_da = xr.DataArray(empty_values, dims=('lat', 'lon'), coords={'lat': np.arange(-60, 85, 0.25), 'lon': np.arange(-180, 180, 0.25)})\n",
    "\n",
    "for sim in sims:\n",
    "    for sens_ind in sens_inds_S1_ST:\n",
    "        ds_sens_ind.update({sens_ind + '_' + sim: empty_da})\n",
    "        \n",
    "sims_S2 = ['RES_IND', 'RES_TRA', 'RES_AGR', 'RES_ENE', 'IND_TRA', 'IND_AGR', 'IND_ENE', 'TRA_AGR', 'TRA_ENE', 'AGR_ENE']\n",
    "sims_S2_indexes = [(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
    "sens_inds_S2 = ['S2', 'S2_conf']\n",
    "\n",
    "for sim in sims_S2:\n",
    "    for sens_ind in sens_inds_S2:\n",
    "        ds_sens_ind.update({sens_ind + '_' + sim: empty_da})\n",
    "\n",
    "sens_inputs = {\n",
    "    'num_vars': 5,\n",
    "    'names': sims,\n",
    "    'bounds': [[0.0, 1.5],\n",
    "               [0.0, 1.5],\n",
    "               [0.0, 1.5],\n",
    "               [0.0, 1.5],\n",
    "               [0.0, 1.5]]\n",
    "}\n",
    "\n",
    "sens_param_values = saltelli.sample(sens_inputs, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_values(gridcell, df_train, df_test, output):\n",
    "    \"\"\"for a given gridcell, return the training and test data\"\"\"\n",
    "    lat, lon = gridcell\n",
    "    \n",
    "    df_train_gridcell = df_train.loc[df_train.lat == lat].loc[df_train.lon == lon]\n",
    "    df_test_gridcell = df_test.loc[df_test.lat == lat].loc[df_test.lon == lon]\n",
    "    \n",
    "    y_train, y_test = df_train_gridcell[output].values, df_test_gridcell[output].values\n",
    "    \n",
    "    return lat, lon, y_train, y_test\n",
    "\n",
    "def create_emulator():\n",
    "    \"\"\"create a new gaussian process emulator\"\"\"\n",
    "    emulator = make_pipeline(\n",
    "        PowerTransformer(),\n",
    "        GaussianProcessRegressor(\n",
    "            kernel=Matern(length_scale=3.4000000000000004, nu=2.5), \n",
    "            n_restarts_optimizer=240, \n",
    "            normalize_y=False)\n",
    "    )\n",
    "    set_param_recursive(emulator.steps, 'random_state', 123)\n",
    "    \n",
    "    return emulator\n",
    "\n",
    "\n",
    "def emulator_cv(emulator, X_train, y_train, y_test):\n",
    "    \"\"\"10-fold cross-validation on the emulator using the training data\"\"\"\n",
    "    cv = cross_validate(emulator, X_train, y_train, cv=10, scoring={'r2': 'r2', 'rmse': 'neg_mean_squared_error'})\n",
    "    \n",
    "    return cv\n",
    "\n",
    "\n",
    "def emulator_fit_save(emulator, X_train, y_train, path, output, lat, lon):\n",
    "    \"\"\"fit the emulator to the training data and save\"\"\"\n",
    "    emulator.fit(X_train, y_train)\n",
    "        \n",
    "    joblib.dump(emulator, path + output + '/emulator_' + output + '_' + str(lat) + '_' + str(lon) + '.joblib')\n",
    "    \n",
    "    return emulator\n",
    "    \n",
    "    \n",
    "def sensitivity_analysis(sens_inputs, sens_predictions):\n",
    "    \"\"\"determine the sensitivity indices of the emulator\"\"\"\n",
    "    sens_ind_dict = sobol.analyze(sens_inputs, sens_predictions)\n",
    "    \n",
    "    return sens_ind_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emulator_with_sensitivity(gridcell):\n",
    "    \"\"\"run all the functions to create the emulator and run the sensitivity analysis\"\"\"\n",
    "    lat, lon, y_train, y_test = output_values(gridcell, df_train, df_test, output)\n",
    "    \n",
    "    emulator = create_emulator()\n",
    "    \n",
    "    cv = emulator_cv(emulator, X_train, y_train, y_test)\n",
    "    r2_cv = cv['test_r2']\n",
    "    rmse_cv = np.sqrt(np.abs(cv['test_rmse']))\n",
    "    \n",
    "    emulator = emulator_fit_save(emulator, X_train, y_train, path, output, lat, lon)\n",
    "    \n",
    "    y_pred = emulator.predict(X_test)\n",
    "    \n",
    "    sens_predictions = emulator.predict(sens_param_values)\n",
    "    sens_ind_dict = sensitivity_analysis(sens_inputs, sens_predictions)\n",
    "\n",
    "    return lat, lon, output, y_test, y_pred, rmse_cv, r2_cv, sens_ind_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'PM2_5_DRY'\n",
    "# ['PM2_5_DRY', 'o3', 'AOD550_sfc', 'asoaX_2p5', 'bc_2p5', 'bsoaX_2p5', \n",
    "# 'nh4_2p5', 'no3_2p5', 'oc_2p5', 'oin_2p5', 'so4_2p5']\n",
    "\n",
    "bag_gridcells = db.from_sequence(df_train[['lat', 'lon']].drop_duplicates().values.tolist())\n",
    "bag_gridcells = bag_gridcells.map(emulator_with_sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_gridcells.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = bag_gridcells.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([result[3] for result in results]).ravel()\n",
    "y_pred = np.array([result[4] for result in results]).ravel()\n",
    "np.savez_compressed(path + output + '/y_test_pred_' + output + '.npz', y_test=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv = np.mean(np.array([result[5] for result in results]))\n",
    "r2_cv = np.mean(np.array([result[6] for result in results]))\n",
    "rmse_test = np.round(np.sqrt(np.abs(mean_squared_error(y_test, y_pred))), decimals=4)\n",
    "r2_test = np.round(r2_score(y_test, y_pred), decimals=4)\n",
    "\n",
    "df_eval_summary = df_eval_summary.append([{\n",
    "        'output': output,\n",
    "        'rmse_cv': rmse_cv, \n",
    "        'r2_cv': r2_cv,                                             \n",
    "        'rmse_test': rmse_test, \n",
    "        'r2_test': r2_test}],              \n",
    "        ignore_index=True, \n",
    "        sort=False)\n",
    "df_eval_summary.to_csv(path + output + '/df_eval_summary_' + output + '.csv')\n",
    "df_eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array([result[0] for result in results]).ravel()\n",
    "lons = np.array([result[1] for result in results]).ravel()\n",
    "sensitivities = [result[7] for result in results]\n",
    "\n",
    "for index, sens in enumerate(sensitivities):\n",
    "    lat = lats[index]\n",
    "    lon = lons[index]\n",
    "\n",
    "    for sim_index, sim in enumerate(sims):\n",
    "        for sens_ind_index, sens_ind in enumerate(sens_inds_S1_ST):\n",
    "            ds_sens_ind[sens_ind + '_' + sim] = xr.where(\n",
    "                (ds_sens_ind.coords['lat'] == lat) & (ds_sens_ind.coords['lon'] == lon),\n",
    "                sens[sens_ind][sim_index],\n",
    "                ds_sens_ind[sens_ind + '_' + sim]\n",
    "            )\n",
    "\n",
    "    for sim_index, sim in enumerate(sims_S2):\n",
    "        for sens_ind_index, sens_ind in enumerate(sens_inds_S2):\n",
    "            ds_sens_ind[sens_ind + '_' + sim] = xr.where(\n",
    "                (ds_sens_ind.coords['lat'] == lat) & (ds_sens_ind.coords['lon'] == lon),\n",
    "                sens[sens_ind][sims_S2_indexes[sim_index][0], sims_S2_indexes[sim_index][1]],\n",
    "                ds_sens_ind[sens_ind + '_' + sim]\n",
    "            )\n",
    "\n",
    "ds_sens_ind.to_netcdf(path + output + '/ds_sens_ind_' + output + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(index, output, df_eval_summary, label, y_test, y_pred):\n",
    "    ax = fig.add_subplot(gs[index])\n",
    "    ax.set_facecolor('whitesmoke')\n",
    "    limit = np.nanmax(y_pred)\n",
    "    plt.xlim([0, limit])\n",
    "    plt.ylim([0, limit])\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xlabel('Simulator, ' + label, fontsize=14)\n",
    "    plt.ylabel('Emulator, ' + label, fontsize=14)\n",
    "    plt.scatter(np.vstack(y_test), np.vstack(y_pred))\n",
    "    x = np.arange(2 * np.ceil(limit))\n",
    "    plt.plot(x, x, '', color='grey', ls='--')\n",
    "    plt.plot(x, 0.5 * x, '', color='grey', ls='--')\n",
    "    plt.plot(x, 2 * x, '', color='grey', ls='--')\n",
    "    text = \"R$^2$ CV = \" + str(np.round(np.nanmean(df_eval_summary['r2_cv'].values[0]), decimals=4)) + \\\n",
    "           \"\\nRMSE CV = \" + str(np.round(np.nanmean(df_eval_summary['rmse_cv'].values[0]), decimals=4)) + \\\n",
    "           \"\\nR$^2$ test = \" + str(np.round(np.nanmean(df_eval_summary['r2_test'].values[0]), decimals=4)) + \\\n",
    "           \"\\nRMSE test = \" + str(np.round(np.nanmean(df_eval_summary['rmse_test'].values[0]), decimals=4))\n",
    "    at = matplotlib.offsetbox.AnchoredText(text, prop=dict(size=11), frameon=True, loc='upper left')\n",
    "    at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "    ax.add_artist(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(6, 6))\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "path = '/nobackup/earlacoa/machinelearning/data/'\n",
    "outputs = ['PM2_5_DRY']#, 'o3']#, 'AOD550_sfc', 'asoaX_2p5']\n",
    "labels = ['ambient PM$_{2.5}$ concentration\\n(${\\mu}g$ $m^{-3}$)',\n",
    "          'ambient O$_{3}$ concentration\\n($ppbv$)',\n",
    "          'surface AOD 550 nm',\n",
    "          'anthropogenic SOA$_{2.5}$ concentration\\n(${\\mu}g$ $m^{-3}$)']\n",
    "pattern = r'([+-]?\\d+.?\\d+)'\n",
    "\n",
    "for index, output in enumerate(outputs):\n",
    "    y_values = np.load(path + output + '/y_test_pred_' + output + '.npz')\n",
    "    y_test = y_values['y_test']\n",
    "    y_pred = y_values['y_pred']\n",
    "    df_eval_summary = pd.read_csv(path + output + '/df_eval_summary_' + output + '.csv')   \n",
    "    make_plot(index, output, df_eval_summary, labels[index], y_test, y_pred)\n",
    "\n",
    "gs.tight_layout(fig, rect=[0, 0, 1.0, 1.0])\n",
    "plt.savefig(path + output + '/' + output + '_eval.png', dpi=700, alpha=True, bbox_inches='tight')\n",
    "plt.savefig(path + output + '/' + output + '_eval.eps', format='eps', dpi=700, alpha=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sens_ind = xr.open_dataset(path + output + '/ds_sens_ind_' + output + '.nc')\n",
    "lon = ds_sens_ind['S1_RES'].lon.values\n",
    "lat = ds_sens_ind['S1_RES'].lat.values\n",
    "xx, yy = np.meshgrid(lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(12, 8))\n",
    "gs = gridspec.GridSpec(2, 3)\n",
    "\n",
    "sens = 'S1'\n",
    "region = 'GBA'\n",
    "\n",
    "label = 'ambient PM$_{2.5}$ concentrations'\n",
    "#label = 'ambient O$_{3}$ concentrations'\n",
    "#label = 'surface AOD 550 nm'\n",
    "#label = 'anthropogenic SOA$_{2.5}$ concentrations'\n",
    "\n",
    "sims = ['RES', 'IND', 'TRA', 'AGR', 'ENE']\n",
    "levels = {}\n",
    "levels.update({'China':(0,0.075,0.15,0.225,0.30,0.375,0.45,0.525,0.60,0.675,100000)})\n",
    "levels.update({'GBA':(0,0.075,0.15,0.225,0.30,0.375,0.45,0.525,0.60,0.675,100000)})\n",
    "cmap_colors = {}\n",
    "cmap_colors.update({'S1': ['#ffffcc', '#ffeda0', '#fed976', '#feb24c', '#fd8d3c', '#fc4e2a', '#e31a1c', '#bd0026', '#800026']})\n",
    "cb_label = {}\n",
    "cb_label.update({'S1':'First-order sensitivity indices for ' + label})\n",
    "plots = [(sim, sens, region, levels[region], cmap_colors[sens], cb_label[sens], label) for sim in sims]\n",
    "\n",
    "for index, item in enumerate(plots):\n",
    "    ax = fig.add_subplot(gs[index], projection=ccrs.PlateCarree())\n",
    "    if item[2] == 'China':\n",
    "        ax.set_extent([73, 135, 18, 54], crs=ccrs.PlateCarree())\n",
    "        shape_feature = ShapelyFeature(Reader('/nobackup/earlacoa/health/data/china_taiwan_hongkong_macao.shp').geometries(),\n",
    "                                       ccrs.PlateCarree(), facecolor='none')\n",
    "        ax.patch.set_visible(False)\n",
    "        ax.spines['geo'].set_visible(False)\n",
    "        ax.add_feature(shape_feature, edgecolor='black', linewidth=0.5)\n",
    "    elif item[2] == 'GBA':\n",
    "        ax.set_extent([111.3, 115.5, 21.5, 24.5], crs=ccrs.PlateCarree())\n",
    "        shape_feature1 = ShapelyFeature(Reader('/nobackup/earlacoa/health/data/gadm36_CHN_2.shp').geometries(),\n",
    "                                        ccrs.PlateCarree(), facecolor='none')\n",
    "        shape_feature2 = ShapelyFeature(Reader('/nobackup/earlacoa/health/data/gadm36_HKG_0.shp').geometries(),\n",
    "                                        ccrs.PlateCarree(), facecolor='none')\n",
    "        shape_feature3 = ShapelyFeature(Reader('/nobackup/earlacoa/health/data/gadm36_MAC_0.shp').geometries(),\n",
    "                                        ccrs.PlateCarree(), facecolor='none')\n",
    "        shape_feature4 = ShapelyFeature(Reader('/nobackup/earlacoa/health/data/gadm36_TWN_0.shp').geometries(),\n",
    "                                        ccrs.PlateCarree(), facecolor='none')\n",
    "        ax.add_feature(shape_feature1, edgecolor='black', linewidth=0.5)\n",
    "        ax.add_feature(shape_feature2, edgecolor='black', linewidth=0.5)\n",
    "        ax.add_feature(shape_feature3, edgecolor='black', linewidth=0.5)\n",
    "        ax.add_feature(shape_feature4, edgecolor='black', linewidth=0.5)\n",
    "    norm = matplotlib.colors.Normalize(vmin=item[3][0], vmax=item[3][-2])\n",
    "    cmap = matplotlib.colors.ListedColormap(list(item[4]))\n",
    "    im = ax.contourf(xx, yy, ds_sens_ind[item[1] + '_' + item[0]].values, item[3],\n",
    "                     cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "    #plt.annotate(r'\\textbf{' + chr(97 + index) + '}', xy=(0,1.05), xycoords='axes fraction', fontsize=14, weight='bold')\n",
    "    plt.annotate(chr(97 + index), xy=(0,1.05), xycoords='axes fraction', fontsize=14, weight='bold')\n",
    "    plt.title(item[0], size=14)\n",
    "\n",
    "fig.canvas.draw()\n",
    "gs.tight_layout(fig, rect=[0, 0, 0.95, 0.95], h_pad=1, w_pad=1) \n",
    "\n",
    "plt.draw()\n",
    "    \n",
    "ax_cbar = fig.add_axes([0.97, 0.15, 0.02, 0.7])\n",
    "sm = plt.cm.ScalarMappable(\n",
    "    norm=matplotlib.colors.Normalize(vmin=levels[region][0], vmax=levels[region][-2]),                       \n",
    "    cmap=im.cmap\n",
    ")\n",
    "sm.set_array([])  \n",
    "cb = plt.colorbar(\n",
    "    sm, \n",
    "    cax=ax_cbar, \n",
    "    norm=matplotlib.colors.Normalize(vmin=levels[region][0], vmax=levels[region][-2]),              \n",
    "    cmap=cmap_colors[sens], \n",
    "    ticks=levels[region][0:-1]\n",
    ")\n",
    "cb.set_label(cb_label[sens], size=14)\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.savefig(path + output + '/' + output + '_' + region + '_compare.png', dpi=700, alpha=True, bbox_inches='tight')\n",
    "plt.savefig(path + output + '/' + output + '_' + region + '_compare.eps', format='eps', dpi=700, alpha=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
