{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from tpot.export_utils import set_param_recursive\n",
    "import xarray as xr\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import cloudpickle\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from cartopy.io.shapereader import Reader\n",
    "params = {\n",
    "    'text.latex.preamble': ['\\\\usepackage{gensymb}'],\n",
    "    'axes.grid': False,\n",
    "    'savefig.dpi': 700,\n",
    "    'font.size': 12,\n",
    "    'text.usetex': False,\n",
    "    'figure.figsize': [5, 5],\n",
    "    'font.family': 'serif',\n",
    "}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SGECluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SGECluster(\n",
    "    walltime='12:00:00', \n",
    "    memory='2 G',\n",
    "    resource_spec='h_vmem=2G',\n",
    "    scheduler_options={\n",
    "        'dashboard_address': ':5757',\n",
    "    },\n",
    "    project='admiralty'\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'PM2_5_DRY'\n",
    "path = '/nobackup/earlacoa/machinelearning/data/'\n",
    "\n",
    "with open(path + 'dict_train.pickle', 'rb') as ds:\n",
    "    dict_train = pickle.load(ds)\n",
    "    \n",
    "df_train = pd.concat(dict_train, ignore_index=True)\n",
    "gridcells = df_train[['lat', 'lon']].drop_duplicates().values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create control using emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "    'PM2_5_DRY',\n",
    "    'o3',\n",
    "    'AOD550_sfc',\n",
    "    'asoaX_2p5',\n",
    "    'bc_2p5',\n",
    "    'bsoaX_2p5',\n",
    "    'nh4_2p5',\n",
    "    'no3_2p5',\n",
    "    'oc_2p5',\n",
    "    'oin_2p5',\n",
    "    'so4_2p5'\n",
    "]\n",
    "\n",
    "fraction_res = 1.0\n",
    "fraction_ind = 1.0\n",
    "fraction_tra = 1.0\n",
    "fraction_agr = 1.0\n",
    "fraction_ene = 1.0\n",
    "\n",
    "custom_inputs = np.array([\n",
    "    fraction_res,\n",
    "    fraction_ind,\n",
    "    fraction_tra,\n",
    "    fraction_agr,\n",
    "    fraction_ene\n",
    "]).reshape(1, -1)\n",
    "\n",
    "empty_values = np.empty((580, 1440))\n",
    "empty_values[:] = np.nan\n",
    "\n",
    "for output in outputs:\n",
    "    emulator_files = glob.glob(path + output + '/emulator_' + output + '_*.joblib')\n",
    "    \n",
    "    ds_custom_output = xr.DataArray(\n",
    "        empty_values, \n",
    "        dims=('lat', 'lon'), \n",
    "        coords={'lat': np.arange(-60, 85, 0.25), 'lon': np.arange(-180, 180, 0.25)}\n",
    "    )\n",
    "    \n",
    "    for emulator_file in emulator_files:\n",
    "        lat, lon = [float(item) for item in re.findall(r'\\d+\\.\\d+', emulator_file)]\n",
    "        emulator = joblib.load(emulator_file)\n",
    "        \n",
    "        try:\n",
    "            custom_output = emulator.predict(custom_inputs)\n",
    "            ds_custom_output = xr.where(\n",
    "                (ds_custom_output.coords['lat'] == lat) & (ds_custom_output.coords['lon'] == lon),\n",
    "                custom_output,\n",
    "                ds_custom_output\n",
    "            )\n",
    "        except:\n",
    "            RuntimeError\n",
    "    \n",
    "    ds_custom_output.name = output\n",
    "    ds_custom_output.to_netcdf(\n",
    "        path + 'summary/ds_ctl_' + output + '.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create individual 10% emulators while holding other inputs at 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'PM2_5_DRY'\n",
    "#output = 'o3'\n",
    "\n",
    "emulator_files = glob.glob(path + output + '/emulator_' + output + '_*.joblib')\n",
    "\n",
    "empty_values = np.empty((580, 1440))\n",
    "empty_values[:] = np.nan\n",
    "\n",
    "matrix1 = np.array(np.meshgrid(np.linspace(0, 1.5, 16), 1, 1, 1, 1)).T.reshape(-1, 5)\n",
    "matrix2 = np.array(np.meshgrid(1, np.linspace(0, 1.5, 16), 1, 1, 1)).T.reshape(-1, 5)\n",
    "matrix3 = np.array(np.meshgrid(1, 1, np.linspace(0, 1.5, 16), 1, 1)).T.reshape(-1, 5)\n",
    "matrix4 = np.array(np.meshgrid(1, 1, 1, np.linspace(0, 1.5, 16), 1)).T.reshape(-1, 5)\n",
    "matrix5 = np.array(np.meshgrid(1, 1, 1, 1, np.linspace(0, 1.5, 16))).T.reshape(-1, 5)\n",
    "matrix_stacked = np.vstack((matrix1, matrix2, matrix3, matrix4, matrix5))\n",
    "\n",
    "for matrix in matrix_stacked:\n",
    "    custom_inputs = matrix.reshape(1, -1)\n",
    "    filename = 'RES' + str(np.round(custom_inputs[0][0], decimals=1)) \\\n",
    "                + '_IND' + str(np.round(custom_inputs[0][1], decimals=1)) \\\n",
    "                + '_TRA' + str(np.round(custom_inputs[0][2], decimals=1)) \\\n",
    "                + '_AGR' + str(np.round(custom_inputs[0][3], decimals=1)) \\\n",
    "                + '_ENE' + str(np.round(custom_inputs[0][4], decimals=1))\n",
    "\n",
    "    ds_custom_output = xr.DataArray(\n",
    "        empty_values, \n",
    "        dims=('lat', 'lon'), \n",
    "        coords={'lat': np.arange(-60, 85, 0.25), 'lon': np.arange(-180, 180, 0.25)}\n",
    "    )\n",
    "\n",
    "    for emulator_file in emulator_files:\n",
    "        lat, lon = [float(item) for item in re.findall(r'\\d+\\.\\d+', emulator_file)]\n",
    "        emulator = joblib.load(emulator_file)\n",
    "        custom_output = emulator.predict(custom_inputs)\n",
    "        ds_custom_output = xr.where(\n",
    "            (ds_custom_output.coords['lat'] == lat) & (ds_custom_output.coords['lon'] == lon),\n",
    "            custom_output,\n",
    "            ds_custom_output\n",
    "        )\n",
    "    \n",
    "    ds_custom_output.name = output\n",
    "    \n",
    "    ds_custom_output.to_netcdf(\n",
    "        path + '/summary/ds_' + filename + '_' + output + '.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create 10% emulators - pangeo\n",
    "parallelise over the custom inputs (as these are independent, while the dataset for gridcells are dependent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask bag good practices\n",
    "- no inter-worker communication\n",
    "    - use the bag to load data\n",
    "- minimise IO\n",
    "- cloudpickle functions\n",
    "- CPU % should ramp up to 100% per worker\n",
    "\n",
    "Dask bag features\n",
    "- immutable\n",
    "- multi-processing (by default)\n",
    "- multiple bags need identical partitions (number and size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### method 1\n",
    "- ramps to 2700% for 32 cores\n",
    "- without dataset creation for 20 custom inputs = CPU: 9 secs, Wall: 35 secs\n",
    "- with dataset creation for 20 custom inputs = CPU: 26 secs, Wall: 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_stacked = np.array(np.meshgrid(\n",
    "    np.linspace(0, 1.5, 16), \n",
    "    np.linspace(0, 1.5, 16),\n",
    "    np.linspace(0, 1.5, 16),\n",
    "    np.linspace(0, 1.5, 16),\n",
    "    np.linspace(0, 1.5, 16)\n",
    ")).T.reshape(-1, 5)\n",
    "\n",
    "custom_inputs = [item.reshape(1, -1) for item in matrix_stacked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates of ones already completed\n",
    "custom_inputs_completed_filenames = glob.glob(path + 'summary/ds*')\n",
    "custom_inputs_completed_list = []\n",
    "for custom_inputs_completed_filename in custom_inputs_completed_filenames:\n",
    "    custom_inputs_completed_list.append(\n",
    "        [float(item) for item in re.findall(r'\\d+\\.\\d+', custom_inputs_completed_filename)]\n",
    "    )\n",
    "    \n",
    "custom_inputs_list = []\n",
    "for custom_input in custom_inputs:\n",
    "    custom_inputs_list.append(\n",
    "        [float(item) for item in re.findall(r'[0-9]\\.[0-9]?', str(custom_input))]\n",
    "    )\n",
    "    \n",
    "custom_inputs = [np.array(item).reshape(1, -1) for item in custom_inputs_list if item not in custom_inputs_completed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridcells_china = pd.DataFrame(gridcells, columns=['lat', 'lon']).set_index(['lat', 'lon'])\n",
    "\n",
    "empty_values = np.empty((580, 1440))\n",
    "empty_values[:] = np.nan\n",
    "\n",
    "ds_empty = xr.DataArray(\n",
    "    empty_values, \n",
    "    dims=('lat', 'lon'), \n",
    "    coords={\n",
    "        'lat': np.arange(-60, 85, 0.25), \n",
    "        'lon': np.arange(-180, 180, 0.25)\n",
    "    }\n",
    ")\n",
    "ds_empty = ds_empty.rename(output)\n",
    "df_empty = ds_empty.to_dataframe().reset_index()\n",
    "df_empty = df_empty.set_index(['lat', 'lon']).sort_index()\n",
    "\n",
    "df_empty = df_empty.drop(df_gridcells_china.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emulator(emulator_file):\n",
    "    lat, lon = [float(item) for item in re.findall(r'\\d+\\.\\d+', emulator_file)]\n",
    "    emulator = joblib.load(emulator_file)\n",
    "    return lat, lon, emulator\n",
    "\n",
    "\n",
    "def custom_predict(loaded_emulator, custom_input):\n",
    "    lat, lon, emulator = loaded_emulator\n",
    "    custom_output = emulator.predict(custom_input)[0]\n",
    "    return lat, lon, custom_input, custom_output\n",
    "\n",
    "\n",
    "def create_dataset(result):\n",
    "    lats = [item[0] for item in result]\n",
    "    lons = [item[1] for item in result]\n",
    "    custom_input = [item[2] for item in result][0]\n",
    "    custom_outputs = [item[3] for item in result]\n",
    "    \n",
    "    filename = 'RES' + str(np.round(custom_input[0][0], decimals=1)) \\\n",
    "                + '_IND' + str(np.round(custom_input[0][1], decimals=1)) \\\n",
    "                + '_TRA' + str(np.round(custom_input[0][2], decimals=1)) \\\n",
    "                + '_AGR' + str(np.round(custom_input[0][3], decimals=1)) \\\n",
    "                + '_ENE' + str(np.round(custom_input[0][4], decimals=1))\n",
    "    \n",
    "    df_results = pd.DataFrame([lats, lons, custom_outputs]).T\n",
    "    df_results.columns = ['lat', 'lon', output]\n",
    "    df_results = df_results.set_index(['lat', 'lon']).sort_index()\n",
    "    \n",
    "    df_custom_output = pd.concat([df_empty, df_results]).sort_index()\n",
    "    ds_custom_output = xr.Dataset.from_dataframe(df_custom_output)    \n",
    "    \n",
    "    ds_custom_output.to_netcdf(\n",
    "        path + 'summary/ds_' + filename + '_' + output + '.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_custom_predict = cloudpickle.dumps(custom_predict)\n",
    "depickled_custom_predict = pickle.loads(pickled_custom_predict)\n",
    "\n",
    "pickled_load_emulator = cloudpickle.dumps(load_emulator)\n",
    "depickled_load_emulator = pickle.loads(pickled_load_emulator)\n",
    "\n",
    "pickled_create_dataset = cloudpickle.dumps(create_dataset)\n",
    "depickled_create_dataset = pickle.loads(pickled_create_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'PM2_5_DRY'\n",
    "emulator_files = glob.glob(path + output + '/emulator_' + output + '_*.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_inputs_sample = custom_inputs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_emulators = db.from_sequence(emulator_files).map(depickled_load_emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.21 s, sys: 273 ms, total: 8.49 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# without dataset creation\n",
    "for custom_input in custom_inputs_sample:\n",
    "    results = []\n",
    "    results.append(bag_emulators.map(depickled_custom_predict, custom_input).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 858 ms, total: 27.5 s\n",
      "Wall time: 58.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with dataset creation\n",
    "for custom_input in custom_inputs_sample:\n",
    "    results = []\n",
    "    results.append(bag_emulators.map(depickled_custom_predict, custom_input).compute())\n",
    "    \n",
    "    create_dataset(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### method 2\n",
    "- ramps to 3200% for 32 cores\n",
    "- without dataset creation for 20 custom inputs = CPU: 1 secs, Wall: 10 secs\n",
    "- with dataset creation for 20 custom inputs = CPU: 18 secs, Wall: 25 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_INPUTS = None\n",
    "number_of_inputs = 20\n",
    "\n",
    "def get_emulator_files(file_path, file_pattern='emulator*'):\n",
    "    emulator_files = glob.glob(os.sep.join([file_path, file_pattern]))\n",
    "    return emulator_files\n",
    "\n",
    "def load_emulator(emulator_file):\n",
    "    lat, lon = [float(item) for item in re.findall(r'\\d+\\.\\d+', emulator_file)]\n",
    "    emulator = joblib.load(emulator_file)\n",
    "    return lat, lon, emulator\n",
    "\n",
    "def get_custom_inputs():\n",
    "    custom_inputs = np.array(np.meshgrid(\n",
    "        np.linspace(0, 1.5, 16),\n",
    "        np.linspace(0, 1.5, 16),\n",
    "        np.linspace(0, 1.5, 16),\n",
    "        np.linspace(0, 1.5, 16),\n",
    "        np.linspace(0, 1.5, 16)\n",
    "    )).T.reshape(-1, 5)\n",
    "    custom_inputs = [custom_input.reshape(1, -1) for custom_input in custom_inputs]\n",
    "    return custom_inputs\n",
    "\n",
    "def custom_predicts(emulator_file):\n",
    "    lat, lon, emulator = load_emulator(emulator_file)\n",
    "\n",
    "    def emulator_wrap(custom_input):\n",
    "        return lat, lon, custom_input, emulator.predict(custom_input)[0]\n",
    "\n",
    "    global CUSTOM_INPUTS\n",
    "    if not CUSTOM_INPUTS:\n",
    "        CUSTOM_INPUTS = get_custom_inputs()[0:number_of_inputs]\n",
    "    custom_inputs = CUSTOM_INPUTS\n",
    "\n",
    "    results = []\n",
    "    results = results + list(map(emulator_wrap, custom_inputs))\n",
    "    return results\n",
    "\n",
    "def create_dataset(lats, lons, custom_input, custom_outputs):\n",
    "    filename = 'RES' + str(np.round(custom_input[0][0], decimals=1)) \\\n",
    "                + '_IND' + str(np.round(custom_input[0][1], decimals=1)) \\\n",
    "                + '_TRA' + str(np.round(custom_input[0][2], decimals=1)) \\\n",
    "                + '_AGR' + str(np.round(custom_input[0][3], decimals=1)) \\\n",
    "                + '_ENE' + str(np.round(custom_input[0][4], decimals=1))\n",
    "    \n",
    "    df_results = pd.DataFrame([lats, lons, custom_outputs]).T\n",
    "    df_results.columns = ['lat', 'lon', output]\n",
    "    df_results = df_results.set_index(['lat', 'lon']).sort_index()\n",
    "    \n",
    "    df_custom_output = pd.concat([df_empty, df_results]).sort_index()\n",
    "    ds_custom_output = xr.Dataset.from_dataframe(df_custom_output)    \n",
    "    \n",
    "    ds_custom_output.to_netcdf(\n",
    "        path + 'summary/ds_' + filename + '_' + output + '.nc'\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    output = 'PM2_5_DRY'\n",
    "    data_dir = f'/nobackup/earlacoa/machinelearning/data/{output}/' \n",
    "\n",
    "    emulator_files = get_emulator_files(data_dir)\n",
    "\n",
    "    emulator_bag = db.from_sequence(emulator_files)\n",
    "\n",
    "    results = emulator_bag.map(custom_predicts).compute()\n",
    "\n",
    "    print(f'Does the total results length equal the number of emulators? {len(results) == len(emulator_files)}')\n",
    "    print(f'Does each emulator length equal the number of custom_inputs? {len(results[0]) == number_of_inputs}')\n",
    "    \n",
    "    result = [result[0] for result in results]\n",
    "    lats = [item[0] for item in result]\n",
    "    lons = [item[1] for item in result]\n",
    "    custom_inputs = [item[2] for item in results[0]]\n",
    "\n",
    "    for index, custom_input in enumerate(custom_inputs):\n",
    "        result = [result[index] for result in results]\n",
    "        custom_outputs = [item[3] for item in result]\n",
    "        create_dataset(lats, lons, custom_input, custom_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the total results length equal the number of emulators? True\n",
      "Does each emulator length equal the number of custom_inputs? True\n",
      "CPU times: user 1.22 s, sys: 88.2 ms, total: 1.31 s\n",
      "Wall time: 9.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# without dataset creation\n",
    "if __name__ == '__main__':\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the total results length equal the number of emulators? True\n",
      "Does each emulator length equal the number of custom_inputs? True\n",
      "CPU times: user 18.5 s, sys: 292 ms, total: 18.7 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with dataset creation\n",
    "if __name__ == '__main__':\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
