{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from tpot.export_utils import set_param_recursive\n",
    "import xarray as xr\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from cartopy.io.shapereader import Reader\n",
    "import cloudpickle\n",
    "params = {\n",
    "    'text.latex.preamble': ['\\\\usepackage{gensymb}'],\n",
    "    'axes.grid': False,\n",
    "    'savefig.dpi': 700,\n",
    "    'font.size': 12,\n",
    "    'text.usetex': False,\n",
    "    'figure.figsize': [5, 5],\n",
    "    'font.family': 'serif',\n",
    "}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SGECluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SGECluster(\n",
    "    walltime='12:00:00', \n",
    "    memory='2 G',\n",
    "    resource_spec='h_vmem=2G',\n",
    "    scheduler_options={\n",
    "        'dashboard_address': ':5757',\n",
    "    },\n",
    "    project='admiralty'\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'PM2_5_DRY'\n",
    "path = '/nobackup/earlacoa/machinelearning/data/'\n",
    "\n",
    "with open(path + 'dict_train.pickle', 'rb') as ds:\n",
    "    dict_train = pickle.load(ds)\n",
    "    \n",
    "df_train = pd.concat(dict_train, ignore_index=True)\n",
    "gridcells = df_train[['lat', 'lon']].drop_duplicates().values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create control using emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "    'PM2_5_DRY',\n",
    "    'o3',\n",
    "    'AOD550_sfc',\n",
    "    'asoaX_2p5',\n",
    "    'bc_2p5',\n",
    "    'bsoaX_2p5',\n",
    "    'nh4_2p5',\n",
    "    'no3_2p5',\n",
    "    'oc_2p5',\n",
    "    'oin_2p5',\n",
    "    'so4_2p5'\n",
    "]\n",
    "\n",
    "fraction_res = 1.0\n",
    "fraction_ind = 1.0\n",
    "fraction_tra = 1.0\n",
    "fraction_agr = 1.0\n",
    "fraction_ene = 1.0\n",
    "\n",
    "custom_inputs = np.array([\n",
    "    fraction_res,\n",
    "    fraction_ind,\n",
    "    fraction_tra,\n",
    "    fraction_agr,\n",
    "    fraction_ene\n",
    "]).reshape(1, -1)\n",
    "\n",
    "empty_values = np.empty((580, 1440))\n",
    "empty_values[:] = np.nan\n",
    "\n",
    "for output in outputs:\n",
    "    emulator_files = glob.glob(path + output + '/emulator_' + output + '_*.joblib')\n",
    "    \n",
    "    ds_custom_output = xr.DataArray(\n",
    "        empty_values, \n",
    "        dims=('lat', 'lon'), \n",
    "        coords={'lat': np.arange(-60, 85, 0.25), 'lon': np.arange(-180, 180, 0.25)}\n",
    "    )\n",
    "    \n",
    "    for emulator_file in emulator_files:\n",
    "        lat, lon = [float(item) for item in re.findall(r'\\d+\\.\\d+', emulator_file)]\n",
    "        emulator = joblib.load(emulator_file)\n",
    "        \n",
    "        try:\n",
    "            custom_output = emulator.predict(custom_inputs)\n",
    "            ds_custom_output = xr.where(\n",
    "                (ds_custom_output.coords['lat'] == lat) & (ds_custom_output.coords['lon'] == lon),\n",
    "                custom_output,\n",
    "                ds_custom_output\n",
    "            )\n",
    "        except:\n",
    "            RuntimeError\n",
    "    \n",
    "    ds_custom_output.name = output\n",
    "    ds_custom_output.to_netcdf(\n",
    "        path + 'summary/ds_ctl_' + output + '.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create individual 10% emulators while holding other inputs at 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'PM2_5_DRY'\n",
    "#output = 'o3'\n",
    "\n",
    "emulator_files = glob.glob(path + output + '/emulator_' + output + '_*.joblib')\n",
    "\n",
    "empty_values = np.empty((580, 1440))\n",
    "empty_values[:] = np.nan\n",
    "\n",
    "matrix1 = np.array(np.meshgrid(np.linspace(0, 1.5, 16), 1, 1, 1, 1)).T.reshape(-1, 5)\n",
    "matrix2 = np.array(np.meshgrid(1, np.linspace(0, 1.5, 16), 1, 1, 1)).T.reshape(-1, 5)\n",
    "matrix3 = np.array(np.meshgrid(1, 1, np.linspace(0, 1.5, 16), 1, 1)).T.reshape(-1, 5)\n",
    "matrix4 = np.array(np.meshgrid(1, 1, 1, np.linspace(0, 1.5, 16), 1)).T.reshape(-1, 5)\n",
    "matrix5 = np.array(np.meshgrid(1, 1, 1, 1, np.linspace(0, 1.5, 16))).T.reshape(-1, 5)\n",
    "matrix_stacked = np.vstack((matrix1, matrix2, matrix3, matrix4, matrix5))\n",
    "\n",
    "for matrix in matrix_stacked:\n",
    "    custom_inputs = matrix.reshape(1, -1)\n",
    "    filename = 'RES' + str(np.round(custom_inputs[0][0], decimals=1)) \\\n",
    "                + '_IND' + str(np.round(custom_inputs[0][1], decimals=1)) \\\n",
    "                + '_TRA' + str(np.round(custom_inputs[0][2], decimals=1)) \\\n",
    "                + '_AGR' + str(np.round(custom_inputs[0][3], decimals=1)) \\\n",
    "                + '_ENE' + str(np.round(custom_inputs[0][4], decimals=1))\n",
    "\n",
    "    ds_custom_output = xr.DataArray(\n",
    "        empty_values, \n",
    "        dims=('lat', 'lon'), \n",
    "        coords={'lat': np.arange(-60, 85, 0.25), 'lon': np.arange(-180, 180, 0.25)}\n",
    "    )\n",
    "\n",
    "    for emulator_file in emulator_files:\n",
    "        lat, lon = [float(item) for item in re.findall(r'\\d+\\.\\d+', emulator_file)]\n",
    "        emulator = joblib.load(emulator_file)\n",
    "        custom_output = emulator.predict(custom_inputs)\n",
    "        ds_custom_output = xr.where(\n",
    "            (ds_custom_output.coords['lat'] == lat) & (ds_custom_output.coords['lon'] == lon),\n",
    "            custom_output,\n",
    "            ds_custom_output\n",
    "        )\n",
    "    \n",
    "    ds_custom_output.name = output\n",
    "    \n",
    "    ds_custom_output.to_netcdf(\n",
    "        path + '/summary/ds_' + filename + '_' + output + '.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create 10% emulators - pangeo\n",
    "parallelise over the custom inputs (as these are independent, while the dataset for gridcells are dependent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask bag good practices\n",
    "- no inter-worker communication\n",
    "    - use the bag to load data\n",
    "- minimise IO\n",
    "- cloudpickle functions\n",
    "\n",
    "Dask bag features\n",
    "- immutable\n",
    "- multi-processing (by default)\n",
    "- multiple bags need identical partitions (number and size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_stacked = np.array(np.meshgrid(\n",
    "    np.linspace(0, 1.5, 16), \n",
    "    np.linspace(0, 1.5, 16),\n",
    "    np.linspace(0, 1.5, 16),\n",
    "    np.linspace(0, 1.5, 16),\n",
    "    np.linspace(0, 1.5, 16)\n",
    ")).T.reshape(-1, 5)\n",
    "\n",
    "custom_inputs = [item.reshape(1, -1) for item in matrix_stacked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates of ones already completed\n",
    "custom_inputs_completed_filenames = glob.glob(path + 'summary/ds*')\n",
    "custom_inputs_completed_list = []\n",
    "for custom_inputs_completed_filename in custom_inputs_completed_filenames:\n",
    "    custom_inputs_completed_list.append(\n",
    "        [float(item) for item in re.findall(r'\\d+\\.\\d+', custom_inputs_completed_filename)]\n",
    "    )\n",
    "    \n",
    "custom_inputs_list = []\n",
    "for custom_input in custom_inputs:\n",
    "    custom_inputs_list.append(\n",
    "        [float(item) for item in re.findall(r'[0-9]\\.[0-9]?', str(custom_input))]\n",
    "    )\n",
    "    \n",
    "custom_inputs = [np.array(item).reshape(1, -1) for item in custom_inputs_list if item not in custom_inputs_completed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_inputs_sample = custom_inputs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'PM2_5_DRY'\n",
    "emulator_files = glob.glob(path + output + '/emulator_' + output + '_*.joblib')\n",
    "\n",
    "def load_emulator(emulator_file):\n",
    "    lat, lon = [float(item) for item in re.findall(r'\\d+\\.\\d+', emulator_file)]\n",
    "    emulator = joblib.load(emulator_file)\n",
    "    return lat, lon, emulator\n",
    "\n",
    "\n",
    "def custom_predict(emulator, custom_input):\n",
    "    lat, lon, emulator = emulator\n",
    "    custom_output = emulator.predict(custom_input)[0]\n",
    "    return lat, lon, custom_input, custom_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_custom_predict = cloudpickle.dumps(custom_predict)\n",
    "depickled_custom_predict = pickle.loads(pickled_custom_predict)\n",
    "\n",
    "pickled_load_emulator = cloudpickle.dumps(load_emulator)\n",
    "depickled_load_emulator = pickle.loads(pickled_load_emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_emulators = db.from_sequence(emulator_files).map(depickled_load_emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 405 ms, sys: 16 ms, total: 421 ms\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = bag_emulators.map(depickled_custom_predict, custom_input).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.65 s, sys: 264 ms, total: 8.91 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "for custom_input in custom_inputs_sample:\n",
    "    results.append(bag_emulators.map(depickled_custom_predict, custom_input).compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cannot use multiple bags as cannot have identical paritions between gridcells (15,372) and inputs (1,000,000)\n",
    "\n",
    "if run in serial for 400 ms each input, then 100 hours for 1,000,000 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_options = []\n",
    "for custom_input in custom_inputs_sample:\n",
    "    for emulator_file in emulator_files:\n",
    "        lat, lon = [float(item) for item in re.findall(r'\\d+\\.\\d+', emulator_file)]\n",
    "        options = {\n",
    "            'custom_input': custom_input,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'emulator_file': emulator_file,\n",
    "        }\n",
    "        mp_options.append(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(options):\n",
    "    custom_input = options['custom_input']\n",
    "    emulator = joblib.load(options['emulator_file'])\n",
    "    custom_output = emulator.predict(custom_input)[0]\n",
    "    return options['lat'], options['lon'], custom_input, custom_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_custom_predict = cloudpickle.dumps(custom_predict)\n",
    "depickled_custom_predict = pickle.loads(pickled_custom_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_mp_options = db.from_sequence(mp_options)\n",
    "bag_mp_options = bag_mp_options.map(depickled_custom_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 2.97 s, total: 1min 15s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = bag_mp_options.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after create datasets from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(results, path + 'results_' + output + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = joblib.load(path + 'results_' + output + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lats = [item[0] for item in results]\n",
    "results_lons = [item[1] for item in results]\n",
    "results_custom_inputs = [item[2][0] for item in results]\n",
    "results_custom_outputs = [item[3] for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_values = np.empty((580, 1440))\n",
    "empty_values[:] = np.nan\n",
    "\n",
    "for custom_input_index, custom_input in enumerate(results_custom_inputs):\n",
    "    filename = 'RES' + str(np.round(custom_input[0][0], decimals=1)) \\\n",
    "                + '_IND' + str(np.round(custom_input[0][1], decimals=1)) \\\n",
    "                + '_TRA' + str(np.round(custom_input[0][2], decimals=1)) \\\n",
    "                + '_AGR' + str(np.round(custom_input[0][3], decimals=1)) \\\n",
    "                + '_ENE' + str(np.round(custom_input[0][4], decimals=1))\n",
    "    \n",
    "    ds_custom_output = xr.DataArray(\n",
    "        empty_values, \n",
    "        dims=('lat', 'lon'), \n",
    "        coords={\n",
    "            'lat': np.arange(-60, 85, 0.25), \n",
    "            'lon': np.arange(-180, 180, 0.25)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(filename)\n",
    "    for custom_output_index, custom_output in enumerate(results_custom_outputs[custom_input_index]):\n",
    "        lat = results_lats[custom_input_index][custom_output_index]\n",
    "        lon = results_lons[custom_input_index][custom_output_index]\n",
    "        ds_custom_output.loc[dict(lat=lat, lon=lon)] = custom_output\n",
    "        \n",
    "    \n",
    "    ds_custom_output.name = output\n",
    "    ds_custom_output.to_netcdf(\n",
    "        path + 'summary/ds_' + filename + '_' + output + '.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
